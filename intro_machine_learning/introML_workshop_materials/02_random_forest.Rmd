---
title: "Introduction to Machine Learning - Part 2"
subtitle: "Random Forest"
author: "Gladstone Bioinformatics Core"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction to Random Forest

## From Decision Trees to Random Forests

### What is a Decision Tree?

A decision tree makes predictions by learning simple decision rules from the data:

```
                    [Temperature > 37.5?]
                    /                    \
                  Yes                     No
                  /                        \
        [Has Cough?]                  [Loss of Smell?]
         /       \                      /         \
       Yes       No                   Yes          No
        |         |                    |            |
    Positive  Negative             Positive     Negative
```

### What is Random Forest?

Random Forest is an **ensemble method** that builds many decision trees and combines their predictions!

```
   Tree 1      Tree 2      Tree 3     ...    Tree N
     |           |           |                  |
  Positive   Negative    Positive           Positive
     \           |           /                  /
      \          |          /                  /
       ========= VOTE ======================= 
                  |
              POSITIVE (majority wins!)
```

### Why "Random"?

Two sources of randomness make trees diverse:
1. **Bootstrap sampling**: Each tree trained on random subset of data
2. **Feature sampling**: Each split considers random subset of features

This diversity reduces overfitting and improves predictions!

---

# Load Libraries and Data

```{r libraries}
# Core libraries
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)

# Set seed for reproducibility
set.seed(123)
```

```{r load-data}
# Load the cleaned dataset from Part 1
covid_ml <- read.csv("data/covid_ml_clean.csv")

# Convert target to factor
covid_ml$covid19_test_results <- as.factor(covid_ml$covid19_test_results)

cat("Dataset dimensions:", dim(covid_ml), "\n")
```

---

# Prepare Data

## Train-Test Split

**Why split the data?**

- **Training set**: Used to train/fit the model
- **Test set**: Used to evaluate model performance on unseen data

This helps us estimate how well our model will generalize to new data!

```{r train-test-split}
# Split data: 70% training, 30% testing
train_index <- createDataPartition(covid_ml$covid19_test_results, 
                                   p = 0.7, 
                                   list = FALSE)

train_data <- covid_ml[train_index, ]
test_data <- covid_ml[-train_index, ]

cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
```

**Note:** Unlike some other algorithms, we don't need to scale features for Random Forest! Tree-based methods are invariant to monotonic transformations of features.

---

# Train a Simple Random Forest

Let's start with a basic Random Forest to understand the output:

```{r simple-rf}
# Train a simple random forest
rf_simple <- randomForest(
  covid19_test_results ~ .,  # Predict target using all features
  data = train_data,
  ntree = 100,               # Number of trees
  importance = TRUE          # Calculate variable importance
)

# View the model summary
print(rf_simple)
```

## Understanding the Output

```{r rf-output-explanation}
# OOB (Out-of-Bag) Error
cat("OUT-OF-BAG (OOB) ERROR EXPLAINED:\n")
cat("================================\n")
cat("Random Forest uses bootstrap sampling, so ~37% of data is left out for each tree.\n")
cat("These 'out-of-bag' samples are used to estimate error without needing a separate validation set!\n\n")

cat("OOB Error Rate:", round(rf_simple$err.rate[nrow(rf_simple$err.rate), "OOB"] * 100, 2), "%\n")
```

## Plot Error vs Number of Trees

```{r error-plot}
# Plot OOB error as more trees are added
plot(rf_simple, main = "Random Forest Error vs Number of Trees")
legend("topright", 
       legend = colnames(rf_simple$err.rate),
       col = 1:3, 
       lty = 1:3)
```

The error typically stabilizes after a certain number of trees. More trees rarely hurt (except for computation time)!

---

# Tune Random Forest with Cross-Validation

## Key Hyperparameters

- **ntree**: Number of trees (more is usually better, but diminishing returns)
- **mtry**: Number of features to consider at each split
  - Default for classification: sqrt(number of features)

```{r tune-rf}
# Set up cross-validation
cv_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Grid of mtry values to try
mtry_grid <- expand.grid(mtry = c(2, 4, 6, 8, 10))

# Train with cross-validation
rf_cv <- train(
  covid19_test_results ~ .,
  data = train_data,
  method = "rf",
  trControl = cv_control,
  tuneGrid = mtry_grid,
  ntree = 200,
  metric = "ROC"
)

# View results
rf_cv
```

```{r plot-tune}
# Plot mtry vs performance
plot(rf_cv, main = "Random Forest: Tuning mtry")
```

```{r best-mtry}
cat("Best mtry:", rf_cv$bestTune$mtry, "\n")
```

---

# Variable Importance

One of the best features of Random Forest is built-in variable importance!

## Mean Decrease Gini

This measures how much each variable contributes to the purity of nodes in the trees. Higher values = more important.

```{r var-importance}
# Get variable importance from our simple RF model
importance_df <- as.data.frame(importance(rf_simple))
importance_df$Variable <- rownames(importance_df)

# View importance metrics
head(importance_df)
```

```{r importance-plot}
# Plot variable importance
varImpPlot(rf_simple, 
           type = 2,  # Mean Decrease Gini
           main = "Variable Importance - Random Forest",
           n.var = 15)
```

```{r importance-ggplot}
# Create a nicer ggplot version
importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  head(15) %>%
  ggplot(aes(x = reorder(Variable, MeanDecreaseGini), 
             y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "#3498db") +
  coord_flip() +
  labs(title = "Top 15 Important Features",
       subtitle = "Based on Mean Decrease in Gini",
       x = "Variable",
       y = "Importance") +
  theme_minimal()
```

---

# Make Predictions

```{r predictions}
# Predict on test set
rf_predictions <- predict(rf_cv, newdata = test_data)

# View predictions
head(rf_predictions)
```

```{r predict-proba}
# Get prediction probabilities
rf_proba <- predict(rf_cv, newdata = test_data, type = "prob")
head(rf_proba)
```

---

# Evaluate Model Performance

## Confusion Matrix

A confusion matrix shows how well the model classified each class:

```
                    Predicted
                 Negative  Positive
Actual Negative    TN        FP
       Positive    FN        TP

TN = True Negative  (correctly predicted negative)
TP = True Positive  (correctly predicted positive)
FN = False Negative (missed positive case)
FP = False Positive (false alarm)
```

```{r confusion-matrix}
# Create confusion matrix
cm_rf <- confusionMatrix(rf_predictions, test_data$covid19_test_results, 
                         positive = "Positive")
print(cm_rf)
```

## Understanding the Metrics

```{r metrics-explanation}
# Extract key metrics
accuracy <- cm_rf$overall["Accuracy"]
sensitivity <- cm_rf$byClass["Sensitivity"]
specificity <- cm_rf$byClass["Specificity"]

cat("KEY METRICS:\n")
cat("============\n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Overall correct predictions\n\n")

cat("Sensitivity (Recall):", round(sensitivity * 100, 2), "%\n")
cat("  - Of all actual positives, how many did we catch?\n")
cat("  - Important when missing a positive case is costly\n\n")

cat("Specificity:", round(specificity * 100, 2), "%\n")
cat("  - Of all actual negatives, how many did we correctly identify?\n")
```

## Visualize Confusion Matrix

```{r cm-plot}
# Convert confusion matrix to data frame for plotting
cm_df <- as.data.frame(cm_rf$table)

ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8, color = "white") +
  scale_fill_gradient(low = "#27ae60", high = "#c0392b") +
  labs(title = "Confusion Matrix - Random Forest",
       x = "Actual",
       y = "Predicted") +
  theme_minimal() +
  theme(axis.text = element_text(size = 12))
```

---

# Summary

## What We Learned

1. ✅ **Random Forest** builds many decision trees and combines their predictions
2. ✅ **Bootstrap + feature sampling** creates diverse trees that reduce overfitting
3. ✅ **No scaling required** - trees are scale-invariant
4. ✅ **Variable importance** helps understand which features matter most
5. ✅ **OOB error** provides built-in validation estimate
6. ✅ **Cross-validation** helps tune hyperparameters like mtry

## When to Use Random Forest

- ✅ Mixed numeric and categorical features
- ✅ Want to understand feature importance
- ✅ Large datasets
- ✅ Don't want to tune many hyperparameters
- ✅ Need a robust baseline model

## What's Next?

In **Part 3**, we'll explore **K-means clustering** - an unsupervised learning method for finding patterns without labels!

---

# Session Info

```{r session-info}
sessionInfo()
```

