---
title: "Introduction to Machine Learning - Part 4"
subtitle: "K-means Clustering"
author: "Gladstone Bioinformatics Core"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction to K-means Clustering

## Supervised vs Unsupervised Learning

So far, we've worked with **supervised learning** (KNN, Random Forest):
- We had labeled data (COVID positive/negative)
- Goal: Predict labels for new data

Now, we'll explore **unsupervised learning**:
- No labels!
- Goal: Find hidden patterns/groups in the data

```
SUPERVISED                    UNSUPERVISED
===========                   ============
Training: ðŸ”´ðŸ”´ðŸ”µðŸ”µ            Training: âšªâšªâšªâšª
(with labels)                 (no labels)
           â†“                            â†“
Model learns                  Algorithm finds
decision boundary             natural groupings
           â†“                            â†“
Predict: ðŸ”´ or ðŸ”µ             Group: A, B, or C
```

## How K-means Works

K-means groups data into K clusters by minimizing within-cluster variance:

```
Step 1: Initialize K centroids randomly
        
        âšª âšª âšª        â˜… (centroid 1)
        âšª âšª
            âšª âšª
                âšª âšª   â˜… (centroid 2)

Step 2: Assign each point to nearest centroid
        
        ðŸ”´ ðŸ”´ ðŸ”´        â˜…
        ðŸ”´ ðŸ”´
            ðŸ”µ ðŸ”µ
                ðŸ”µ ðŸ”µ   â˜…

Step 3: Update centroids (move to cluster mean)
        
        ðŸ”´ ðŸ”´ ðŸ”´  
        ðŸ”´ â˜…ðŸ”´
            ðŸ”µ ðŸ”µ
                ðŸ”µ â˜…ðŸ”µ  

Step 4: Repeat steps 2-3 until convergence
```

---

# Load Libraries and Data

```{r libraries}
# Core libraries
library(tidyverse)
library(cluster)      # For clustering algorithms
library(factoextra)   # For visualization
library(ggplot2)

# Set seed for reproducibility
set.seed(123)
```

```{r load-data}
# Load the cleaned dataset from Part 1
covid_ml <- read.csv("data/covid_ml_clean.csv")

cat("Dataset dimensions:", dim(covid_ml), "\n")
head(covid_ml)
```

---

# Prepare Data for K-means

## Important Considerations

âš ï¸ **K-means limitations:**

1. **Cannot handle missing values** - must impute or remove first
2. **Euclidean distance** - assumes numeric, continuous data
3. **Sensitive to scale** - must standardize features

## Select Features for Clustering

Since our data is mostly binary (0/1) symptoms, let's discuss the implications:

```{r feature-selection}
# For K-means, we'll use only numeric features
# We'll exclude the target variable (since it's unsupervised!)

cluster_data <- covid_ml %>%
  select(-covid19_test_results)  # Remove target

cat("Features for clustering:\n")
names(cluster_data)
```

```{r data-types}
# Check data types
cat("\nData types:\n")
sapply(cluster_data, class)
```

## Discussion: K-means with Binary Data

K-means uses **Euclidean distance**, which treats:
- 0 â†’ 1 as distance = 1
- 1 â†’ 0 as distance = 1

For binary data, this is mathematically valid but may not capture the best similarity. Alternatives include:
- **K-modes**: Designed for categorical data
- **Gower distance**: Handles mixed data types
- **Jaccard distance**: For binary presence/absence data

For this workshop, we'll proceed with K-means on our binary + numeric data, but keep these limitations in mind!

## Scale the Data

```{r scale-data}
# Scale all features
cluster_scaled <- scale(cluster_data)

# Verify scaling
cat("Scaled data - Column means (should be ~0):\n")
round(colMeans(cluster_scaled), 10)[1:5]

cat("\nScaled data - Column SDs (should be ~1):\n")
round(apply(cluster_scaled, 2, sd), 10)[1:5]
```

---

# Choosing K: The Elbow Method

The hardest part of K-means: **How many clusters?**

## The Elbow Method

We plot **within-cluster sum of squares (WSS)** vs K:
- WSS measures how compact the clusters are
- As K increases, WSS always decreases
- Look for the "elbow" where the rate of decrease slows

```{r elbow-method}
# Calculate WSS for different K values
fviz_nbclust(cluster_scaled, kmeans, method = "wss", k.max = 10) +
  geom_vline(xintercept = 3, linetype = "dashed", color = "red") +
  labs(title = "Elbow Method for Optimal K",
       subtitle = "Look for the 'elbow' where WSS decrease slows") +
  theme_minimal()
```

## Silhouette Method

Another approach: measure how similar points are to their own cluster vs other clusters.

```{r silhouette-method}
# Silhouette method
fviz_nbclust(cluster_scaled, kmeans, method = "silhouette", k.max = 10) +
  labs(title = "Silhouette Method for Optimal K",
       subtitle = "Higher silhouette = better-defined clusters") +
  theme_minimal()
```

## Gap Statistic

Compares within-cluster variation to a null reference distribution.

```{r gap-statistic}
# Gap statistic (this takes a bit longer)
set.seed(123)
gap_stat <- clusGap(cluster_scaled, FUN = kmeans, nstart = 25, K.max = 10, B = 50)

fviz_gap_stat(gap_stat) +
  labs(title = "Gap Statistic for Optimal K") +
  theme_minimal()
```

---

# Run K-means Clustering

Based on our analysis, let's try K = 3 (you can adjust based on the elbow plot):

```{r run-kmeans}
# Run K-means with K = 3
k <- 3

kmeans_result <- kmeans(
  cluster_scaled,
  centers = k,
  nstart = 50,        # Try 50 random starts, keep best
  iter.max = 100,     # Maximum iterations
  algorithm = "Lloyd" # Classic K-means algorithm
)

# View results
cat("K-means Results:\n")
cat("================\n")
cat("Number of clusters:", k, "\n")
cat("Cluster sizes:", kmeans_result$size, "\n")
cat("Within-cluster sum of squares:", round(kmeans_result$tot.withinss, 2), "\n")
cat("Between/Total SS ratio:", round(kmeans_result$betweenss / kmeans_result$totss * 100, 1), "%\n")
```

## Understanding the Output

```{r kmeans-output}
# Cluster assignments
head(kmeans_result$cluster, 20)

# Cluster centers (on scaled data)
cat("\nCluster Centers (scaled):\n")
round(kmeans_result$centers[, 1:6], 3)
```

---

# Visualize Clusters

## PCA-based Visualization

Since we have many features, we'll use PCA to reduce to 2D for visualization:

```{r cluster-viz}
# Visualize clusters (uses PCA internally)
fviz_cluster(
  kmeans_result, 
  data = cluster_scaled,
  palette = c("#e74c3c", "#3498db", "#2ecc71"),
  ellipse.type = "convex",
  star.plot = TRUE,
  repel = TRUE,
  ggtheme = theme_minimal()
) +
  labs(title = paste("K-means Clustering (K =", k, ")"),
       subtitle = "Visualization using first 2 principal components")
```

## Cluster Profiles

Let's understand what makes each cluster different:

```{r cluster-profiles}
# Add cluster assignment to original data
covid_clustered <- covid_ml %>%
  mutate(cluster = as.factor(kmeans_result$cluster))

# Calculate mean of each feature by cluster
cluster_profiles <- covid_clustered %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), mean)) %>%
  pivot_longer(-cluster, names_to = "feature", values_to = "mean")

# Plot heatmap of cluster profiles
ggplot(cluster_profiles, aes(x = cluster, y = feature, fill = mean)) +
  geom_tile() +
  scale_fill_gradient2(low = "#3498db", mid = "white", high = "#e74c3c", midpoint = 0.5) +
  labs(title = "Cluster Profiles",
       subtitle = "Mean value of each feature by cluster",
       x = "Cluster",
       y = "Feature") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8))
```

## Compare Clusters with COVID Status

Even though K-means is unsupervised, we can check if clusters correspond to COVID status:

```{r cluster-vs-covid}
# Cross-tabulate clusters with COVID status
cluster_covid_table <- table(
  Cluster = covid_clustered$cluster,
  COVID_Status = covid_clustered$covid19_test_results
)

print(cluster_covid_table)

# Proportions
cat("\nProportions within each cluster:\n")
prop.table(cluster_covid_table, margin = 1)
```

```{r cluster-covid-plot}
# Visualize
ggplot(covid_clustered, aes(x = cluster, fill = covid19_test_results)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("Negative" = "#2ecc71", "Positive" = "#e74c3c")) +
  labs(title = "COVID Status Distribution by Cluster",
       x = "Cluster",
       y = "Proportion",
       fill = "COVID Status") +
  theme_minimal()
```

---

# Alternative: Gower Distance for Mixed Data

For datasets with mixed types (numeric + categorical), Gower distance is more appropriate:

```{r gower-demo}
# Gower distance can handle mixed data types
library(cluster)

# Calculate Gower distance matrix
# Note: This can be slow for large datasets
gower_dist <- daisy(cluster_data, metric = "gower")

# Run PAM (Partitioning Around Medoids) - like K-means but uses medoids
pam_result <- pam(gower_dist, k = 3)

cat("PAM with Gower Distance:\n")
cat("Cluster sizes:", pam_result$clusinfo[, "size"], "\n")
```

```{r gower-silhouette}
# Silhouette plot for PAM
fviz_silhouette(pam_result) +
  labs(title = "Silhouette Plot - PAM with Gower Distance") +
  theme_minimal()
```

---

# Evaluating Cluster Quality

## Silhouette Analysis

Silhouette values range from -1 to 1:
- **Near 1**: Point is well-matched to its cluster
- **Near 0**: Point is on the border between clusters
- **Negative**: Point might be in wrong cluster

```{r silhouette-analysis}
# Calculate silhouette for K-means result
sil <- silhouette(kmeans_result$cluster, dist(cluster_scaled))

# Summary
cat("Average Silhouette Width:", round(mean(sil[, "sil_width"]), 3), "\n")

# Plot
fviz_silhouette(sil) +
  labs(title = "Silhouette Analysis for K-means") +
  theme_minimal()
```

---

# Summary

## What We Learned

1. âœ… **K-means** is an unsupervised algorithm that groups data into K clusters
2. âœ… **Choosing K** - use elbow method, silhouette, or gap statistic
3. âœ… **Data preparation** - must handle missing values and scale features
4. âœ… **Limitations** - Euclidean distance may not be ideal for categorical/binary data
5. âœ… **Alternatives** - Gower distance + PAM for mixed data types
6. âœ… **Evaluation** - use silhouette scores to assess cluster quality

## K-means vs Other Methods

| Method | Distance | Data Type | Finds |
|--------|----------|-----------|-------|
| K-means | Euclidean | Numeric | Spherical clusters |
| K-medoids (PAM) | Any | Any | Robust to outliers |
| Hierarchical | Any | Any | Nested clusters |
| DBSCAN | Euclidean | Numeric | Arbitrary shapes |

## When to Use K-means

- âœ… Numeric, continuous features
- âœ… Expected spherical/compact clusters
- âœ… Large datasets (efficient)
- âœ… Know approximate number of clusters

## When NOT to Use K-means

- âŒ Categorical features (use K-modes or Gower)
- âŒ Missing data (must handle first)
- âŒ Clusters of varying sizes/densities
- âŒ Non-spherical cluster shapes

---

# Workshop Summary

## What We Covered

| Part | Topic | Type | Key Concepts |
|------|-------|------|--------------|
| 1 | Data Preprocessing | Preparation | Missing values, encoding, scaling |
| 2 | KNN | Supervised | Distance, K selection, cross-validation |
| 3 | Random Forest | Supervised | Ensemble, feature importance, OOB |
| 4 | K-means | Unsupervised | Clustering, elbow method, silhouette |

## Key Takeaways

1. **Always explore and clean your data first!**
2. **Understand your algorithm's requirements** (scaling, missing data, data types)
3. **Use cross-validation** to avoid overfitting
4. **Evaluate appropriately** - accuracy isn't everything!
5. **No algorithm is universally best** - choose based on your data and goals

## Resources for Further Learning

- [An Introduction to Statistical Learning](https://www.statlearning.com/) - Free textbook
- [caret package documentation](https://topepo.github.io/caret/)
- [tidymodels](https://www.tidymodels.org/) - Modern ML framework in R
- [scikit-learn](https://scikit-learn.org/) - Python ML library with great documentation

---

# Session Info

```{r session-info}
sessionInfo()
```

