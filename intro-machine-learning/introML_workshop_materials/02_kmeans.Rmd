---
title: "Introduction to Machine Learning - Part 2"
subtitle: "K-means Clustering"
author: "Gladstone Bioinformatics Core"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction to K-means Clustering

K-means is an **unsupervised method** that groups data into K clusters by minimizing within-cluster variance.

---

# Load Libraries and Data

```{r libraries}
library(cluster)
library(factoextra)
library(ggplot2)
library(dplyr)
library(SamSPECTRAL)

# Set seed for reproducibility
set.seed(123)
```

```{r load-data}
# Load the cleaned dataset from Part 1
covid_ml <- read.csv("data/covid_ml_clean.csv")
head(covid_ml)
```

---

# Prepare Data for K-means

## Important Considerations

⚠️ **K-means requirements:**

1. **Cannot handle missing values** - must impute or remove first (we did this in Part 1!)
2. **Uses Euclidean distance** - works best with numeric data
3. **Sensitive to scale** - must standardize features

## Select Features for Clustering

Since K-means is unsupervised, we'll exclude the target variable. Also, K-means requires numeric input, so we'll use `cough_severity_encoded` instead of the original `cough_severity`:

```{r feature-selection}
# For K-means, we'll use only numeric features
# We'll exclude the target variable (since it's unsupervised!)
# We'll also exclude cough_severity (categorical) and use cough_severity_encoded (numeric) instead

cluster_data <- covid_ml %>%
  select(-covid19_test_results, -cough_severity) %>% # Remove target and categorical version
  as.data.frame()  

cat("Features for clustering:\n")
names(cluster_data)
```

**Note:** K-means requires numeric input, so we use `cough_severity_encoded` (numeric) rather than `cough_severity` (categorical). Random Forest in Part 3 can handle categorical variables directly, so it will use the original `cough_severity`.

## Scale the Data

```{r scale-data}
# Scale all features
cluster_scaled <- scale(cluster_data)

# Verify scaling
cat("Scaled data - Column means (should be ~0):\n")
round(colMeans(cluster_scaled), 10)[1:5]

cat("\nScaled data - Column SDs (should be ~1):\n")
round(apply(cluster_scaled, 2, sd), 10)[1:5]
```

---

# Choosing K: The Elbow Method

The hardest part of K-means: **How many clusters?**

## The Elbow Method

We plot **within-cluster sum of squares (WSS)** vs K:  
- WSS measures how compact the clusters are  
- As K increases, WSS always decreases  
- Look for the "elbow" where the rate of decrease slows  

```{r choose-k}
# Calculate WSS for different K values
fviz.p <- fviz_nbclust(x = cluster_scaled, 
                       FUNcluster = kmeans, 
                       method = "wss", 
                       k.max = 10) +
  labs(title = "Elbow Method for Optimal K",
       subtitle = "Look for the 'elbow' where WSS decrease slows") +
  theme_minimal()

fviz.p
```

## Knee Point Detection

We can also use an automated method to detect the elbow point:

```{r knee-point}
# Extract y values from the plot
fviz.y <- ggplot_build(fviz.p)$data[[1]][,"y"]

# Detect knee point using SamSPECTRAL package
detected <- kneepointDetection(vect = fviz.y)
cat("Detected optimal K (knee point):", detected$MinIndex, "\n")
```

---

# Train K-means

```{r train-kmeans}
# Based on our data, let's try K = 2 (you can adjust based on the elbow plot).
# Train K-means with K = 2
fit <- kmeans(
  cluster_scaled,
  centers = 2,
  nstart = 50,        # Try 50 random starts, keep best
  iter.max = 100,     # Maximum iterations
  algorithm = "Lloyd" # Classic K-means algorithm
)

# View cluster assignments
head(fit$cluster)
```

---

# Visualize Clusters

```{r visualize-clusters}
# Visualize clusters (uses PCA internally for dimensionality reduction)
fviz_cluster(fit, data = cluster_scaled, 
             palette = c("#e74c3c", "#3498db"), 
             ggtheme = theme_minimal())
```

---

# Compare Clusters with COVID Status

Even though K-means is unsupervised, we can check if clusters correspond to COVID test results:

```{r cluster-covid}
# Add cluster assignments to original data
covid_clustered <- covid_ml %>%
  mutate(cluster = as.factor(fit$cluster))

# Cross-tabulate clusters with COVID status
cluster_covid_table <- table(
  Cluster = covid_clustered$cluster,
  COVID_Status = covid_clustered$covid19_test_results
)

print(cluster_covid_table)
```

---

# Summary

In this part, we learned:

1. ✅ **K-means** groups data into K clusters by minimizing within-cluster variance
2. ✅ **Choose K** using the elbow method (look for where WSS decrease slows)
3. ✅ **Scale features** before clustering (K-means is sensitive to scale)
4. ✅ **Visualize clusters** using PCA-based visualization

## What's Next?

In **Part 3**, we'll explore **Random Forest** - a supervised learning method for classification!

---

# Session Info

```{r session-info}
sessionInfo()
```